

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Model Distillation &mdash; Sentence-Transformers  documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
  
  
  
    <link rel="canonical" href="https://www.sbert.netexamples/training/distillation/README.html"/>
  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/js/custom.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Cross-Encoders" href="../cross-encoder/README.html" />
    <link rel="prev" title="Multilingual-Models" href="../multilingual/README.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

            <a href="../../../index.html">
              <img src="../../../_static/logo.png" class="logo" alt="Logo"/>
              <span class="icon icon-home project-name"> Sentence-Transformers</span>
            </a>

            <div style="display: flex; justify-content: center;">
              <div id="twitter-button">
                <a href="https://twitter.com/Nils_Reimers" target="_blank" title="Follow SBERT on Twitter"><img src="/_static/Twitter_Logo_White.svg" height="20" style="margin: 0px 10px 0px -10px;"> </a>
              </div>
              <div id="github-button"></div>
            </div>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/pretrained_models.html">Pretrained Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/pretrained_cross-encoders.html">Pretrained Cross-Encoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/hugging_face.html">Hugging Face ðŸ¤—</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../applications/computing-embeddings/README.html">Computing Sentence Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/usage/semantic_textual_similarity.html">Semantic Textual Similarity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../applications/semantic-search/README.html">Semantic Search</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../applications/retrieve_rerank/README.html">Retrieve &amp; Re-Rank</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../applications/clustering/README.html">Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../applications/paraphrase-mining/README.html">Paraphrase Mining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../applications/parallel-sentence-mining/README.html">Translated Sentence Mining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../applications/cross-encoder/README.html">Cross-Encoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../applications/image-search/README.html">Image Search</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../docs/training/overview.html">Training Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual/README.html">Multilingual-Models</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Model Distillation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#knowledge-distillation">Knowledge Distillation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#speed-performance-trade-off">Speed - Performance Trade-Off</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dimensionality-reduction">Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quantization">Quantization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../cross-encoder/README.html">Cross-Encoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_augmentation/README.html">Augmented SBERT</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../sts/README.html">Semantic Textual Similarity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nli/README.html">Natural Language Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../paraphrases/README.html">Paraphrase Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quora_duplicate_questions/README.html">Quora Duplicate Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ms_marco/README.html">MS MARCO</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Unsupervised Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../unsupervised_learning/README.html">Unsupervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../domain_adaptation/README.html">Domain Adaptation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/SentenceTransformer.html">SentenceTransformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/util.html">util</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/losses.html">Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/evaluation.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/datasets.html">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/cross_encoder.html">cross_encoder</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Sentence-Transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Model Distillation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            
              <a href="https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/README.md" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="model-distillation">
<h1>Model Distillation<a class="headerlink" href="#model-distillation" title="Permalink to this headline">Â¶</a></h1>
<p>This folder contains example to make SentenceTransformer models <strong>faster, cheaper and lighter</strong>. These light models achieve 97.5% - 100% performance of the original model on downstream tasks.</p>
<div class="section" id="knowledge-distillation">
<h2>Knowledge Distillation<a class="headerlink" href="#knowledge-distillation" title="Permalink to this headline">Â¶</a></h2>
<p>See: <strong><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/distillation/model_distillation.py">model_distillation.py</a></strong></p>
<p>Knowledge distillation describes the process to transfer knowledge from a  teacher model to a student model. It can be used to extend sentence embeddings to new languages (<a class="reference external" href="https://arxiv.org/abs/2004.09813">Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation</a>), but the traditional approach is to have slow (but well performing) teacher model and a fast student model.</p>
<p>The fast student model imitates the teacher model and achieves by this a high performance.</p>
<p><img alt="Knowledge Distillation" src="https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/monolingual-distillation.png" /></p>
<p><strong><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/distillation/model_distillation.py">model_distillation.py</a></strong> implements two options for creating the student model:</p>
<ol class="simple">
<li><p>Use a light transformer model like TinyBERT or BERT-Small to imitate the teacher.</p></li>
<li><p>We take the teacher model and keep only certain layers, for example, only 4 layers.</p></li>
</ol>
<p>Option 2) works usually better, as we keep most of the weights from the teacher. In Option 1, we have to tune all
weights in the student from scratch.</p>
</div>
<div class="section" id="speed-performance-trade-off">
<h2>Speed - Performance Trade-Off<a class="headerlink" href="#speed-performance-trade-off" title="Permalink to this headline">Â¶</a></h2>
<p>Smaller models are faster, but show a (slightly) worse performance when evaluated on down stream tasks. To get an impression of this trade-off, we show some numbers of the <em>stsb-roberta-base</em> model with different number of layers:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Layers</th>
<th align="center">STSbenchmark Performance</th>
<th align="center">Performance Decrease</th>
<th align="center">Speed (Sent. / Sec. on V100-GPU)</th>
</tr>
</thead>
<tbody>
<tr>
<td>teacher: 12</td>
<td align="center">85.44</td>
<td align="center">-</td>
<td align="center">2300</td>
</tr>
<tr>
<td>8</td>
<td align="center">85.54</td>
<td align="center">+0.1%</td>
<td align="center">3200</td>
</tr>
<tr>
<td>6</td>
<td align="center">85.23</td>
<td align="center">-0.2%</td>
<td align="center">4000</td>
</tr>
<tr>
<td>4</td>
<td align="center">84.92</td>
<td align="center">-0.6%</td>
<td align="center">5300</td>
</tr>
<tr>
<td>3</td>
<td align="center">84.39</td>
<td align="center">-1.2%</td>
<td align="center">6500</td>
</tr>
<tr>
<td>2</td>
<td align="center">83.32</td>
<td align="center">-2.5%</td>
<td align="center">7700</td>
</tr>
<tr>
<td>1</td>
<td align="center">80.86</td>
<td align="center">-5.4%</td>
<td align="center">9200</td>
</tr>
</tbody>
</table></div>
<div class="section" id="dimensionality-reduction">
<h2>Dimensionality Reduction<a class="headerlink" href="#dimensionality-reduction" title="Permalink to this headline">Â¶</a></h2>
<p>By default, the pretrained models output embeddings with size 768 (base-models) or with size 1024 (large-models). However, when you store Millions of embeddings, this can require quite a lot of memory / storage.</p>
<p><strong><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/distillation/dimensionality_reduction.py">dimensionality_reduction.py</a></strong> contains a simple example how to reduce the embedding dimension to any size by using Principle Component Analysis (PCA). In that example, we reduce 768 dimension to 128 dimension, reducing the storage requirement by factor 6. The performance only slightly drops from 85.44 to 84.96 on the STS benchmark dataset.</p>
<p>This dimensionality reduction technique can easily be applied to existent models. We could even reduce the embeddings size to 32, reducing the storage requirment by factor 24 (performance decreases to 81.82).</p>
<p>Note: This technique neither improves the runtime, nor the memory requirement for running the model. It only reduces the needed space to store embeddings, for example, for <a class="reference internal" href="../../applications/semantic-search/README.html"><span class="doc">semantic search</span></a>.</p>
</div>
<div class="section" id="quantization">
<h2>Quantization<a class="headerlink" href="#quantization" title="Permalink to this headline">Â¶</a></h2>
<p>A <a class="reference external" href="https://pytorch.org/docs/stable/quantization.html">quantized model</a> executes some or all of the operations with integers rather than floating point values. This allows for a more compact models and the use of high performance vectorized operations on many hardware platforms.</p>
<p>For models that are run on <strong>CPUs</strong>, this can yield 40% smaller models and a faster inference time: Dependining on the CPU, speedup are between 15% and 400%. Model quantization is (as of now) not supported for GPUs by PyTorch.</p>
<p>For an example, see <a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/distillation/model_quantization.py">model_quantization.py</a></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../cross-encoder/README.html" class="btn btn-neutral float-right" title="Cross-Encoders" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../multilingual/README.html" class="btn btn-neutral float-left" title="Multilingual-Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2022, Nils Reimers

       &bull; <a href="/docs/contact.html">Contact</a>

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>