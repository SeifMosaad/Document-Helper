

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Retrieve &amp; Re-Rank &mdash; Sentence-Transformers  documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
  
  
  
    <link rel="canonical" href="https://www.sbert.netexamples/applications/retrieve_rerank/README.html"/>
  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/js/custom.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Clustering" href="../clustering/README.html" />
    <link rel="prev" title="Semantic Search" href="../semantic-search/README.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

            <a href="../../../index.html">
              <img src="../../../_static/logo.png" class="logo" alt="Logo"/>
              <span class="icon icon-home project-name"> Sentence-Transformers</span>
            </a>

            <div style="display: flex; justify-content: center;">
              <div id="twitter-button">
                <a href="https://twitter.com/Nils_Reimers" target="_blank" title="Follow SBERT on Twitter"><img src="/_static/Twitter_Logo_White.svg" height="20" style="margin: 0px 10px 0px -10px;"> </a>
              </div>
              <div id="github-button"></div>
            </div>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/pretrained_models.html">Pretrained Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/pretrained_cross-encoders.html">Pretrained Cross-Encoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/hugging_face.html">Hugging Face ðŸ¤—</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Usage</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../computing-embeddings/README.html">Computing Sentence Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/usage/semantic_textual_similarity.html">Semantic Textual Similarity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../semantic-search/README.html">Semantic Search</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Retrieve &amp; Re-Rank</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#retrieve-re-rank-pipeline">Retrieve &amp; Re-Rank Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="#retrieval-bi-encoder">Retrieval: Bi-Encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#re-ranker-cross-encoder">Re-Ranker: Cross-Encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#example-scripts">Example Scripts</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pre-trained-bi-encoders-retrieval">Pre-trained Bi-Encoders (Retrieval)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pre-trained-cross-encoders-re-ranker">Pre-trained Cross-Encoders (Re-Ranker)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../clustering/README.html">Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../paraphrase-mining/README.html">Paraphrase Mining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel-sentence-mining/README.html">Translated Sentence Mining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cross-encoder/README.html">Cross-Encoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../image-search/README.html">Image Search</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/training/overview.html">Training Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/multilingual/README.html">Multilingual-Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/distillation/README.html">Model Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/cross-encoder/README.html">Cross-Encoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/data_augmentation/README.html">Augmented SBERT</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../training/sts/README.html">Semantic Textual Similarity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/nli/README.html">Natural Language Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/paraphrases/README.html">Paraphrase Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/quora_duplicate_questions/README.html">Quora Duplicate Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/ms_marco/README.html">MS MARCO</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Unsupervised Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../unsupervised_learning/README.html">Unsupervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../domain_adaptation/README.html">Domain Adaptation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/SentenceTransformer.html">SentenceTransformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/util.html">util</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/losses.html">Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/evaluation.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/datasets.html">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/cross_encoder.html">cross_encoder</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Sentence-Transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Retrieve &amp; Re-Rank</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            
              <a href="https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/README.md" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="retrieve-re-rank">
<h1>Retrieve &amp; Re-Rank<a class="headerlink" href="#retrieve-re-rank" title="Permalink to this headline">Â¶</a></h1>
<p>In <a class="reference internal" href="../semantic-search/README.html"><span class="doc">Semantic Search</span></a> we have shown how to use SentenceTransformer to compute embeddings for queries, sentences, and paragraphs and how to use this for semantic search.</p>
<p>For complex search tasks, for example, for question answering retrieval, the search can significantly be improved by using <strong>Retrieve &amp; Re-Rank</strong>.</p>
<div class="section" id="retrieve-re-rank-pipeline">
<h2>Retrieve &amp; Re-Rank Pipeline<a class="headerlink" href="#retrieve-re-rank-pipeline" title="Permalink to this headline">Â¶</a></h2>
<p>A pipeline for information retrieval / question answering retrieval that works well is the following. All components are provided and explained in this article:</p>
<p><img alt="InformationRetrieval" src="https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/InformationRetrieval.png" /></p>
<p>Given a search query, we first use a <strong>retrieval system</strong> that retrieves a large list of e.g. 100 possible hits which are potentially relevant for the query. For the retrieval, we can use either lexical search, e.g. with ElasticSearch, or we can use dense retrieval with a bi-encoder.</p>
<p>However, the retrieval system might retrieve documents that are not that relevant for the search query. Hence, in a second stage, we use a <strong>re-ranker</strong> based on a <strong>cross-encoder</strong> that scores the relevancy of all candidates for the given search query.</p>
<p>The output will be a ranked list of hits we can present to the user.</p>
</div>
<div class="section" id="retrieval-bi-encoder">
<h2>Retrieval: Bi-Encoder<a class="headerlink" href="#retrieval-bi-encoder" title="Permalink to this headline">Â¶</a></h2>
<p>For the retrieval of the candidate set, we can either use lexical search (e.g. <a class="reference external" href="https://www.elastic.co/elasticsearch/">ElasticSearch</a>), or we can use a bi-encoder which is implemented in this repository.</p>
<p>Lexical search looks for literal matches of the query words in your document collection. It will not recognize synonyms, acronyms or spelling variations. In contrast, semantic search (or dense retrieval) encodes the search query into vector space and retrieves the document embeddings that are close in vector space.</p>
<p><img alt="SemanticSearch" src="https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/SemanticSearch.png" /></p>
<p>Semantic search overcomes the short comings of lexical search and can recognize synonym and acronyms. Have a look at the <a class="reference internal" href="../semantic-search/README.html"><span class="doc">semantic search article</span></a>  for different options to implement semantic search.</p>
</div>
<div class="section" id="re-ranker-cross-encoder">
<h2>Re-Ranker: Cross-Encoder<a class="headerlink" href="#re-ranker-cross-encoder" title="Permalink to this headline">Â¶</a></h2>
<p>The retriever has to be efficient for large document collections with millions of entries. However, it might return irrelevant candidates.</p>
<p>A re-ranker based on a Cross-Encoder can substantially improve the final results for the user. The query and a possible document is passed simultaneously to transformer network, which then outputs a single score between 0 and 1 indicating how relevant the document is for the given query.</p>
<p><img alt="CrossEncoder" src="https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/CrossEncoder.png" /></p>
<p>The advantage of Cross-Encoders is the higher performance, as they perform attention across the query and the document.</p>
<p>Scoring thousands or millions of (query, document)-pairs would be rather slow. Hence, we use the retriever to create a set of e.g. 100 possible candidates which are then re-ranked by the Cross-Encoder.</p>
</div>
<div class="section" id="example-scripts">
<h2>Example Scripts<a class="headerlink" href="#example-scripts" title="Permalink to this headline">Â¶</a></h2>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb">retrieve_rerank_simple_wikipedia.ipynb</a></strong> [ <a class="reference external" href="https://colab.research.google.com/github/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb">Colab Version</a> ]: This script uses the smaller <a class="reference external" href="https://simple.wikipedia.org/wiki/Main_Page">Simple English Wikipedia</a> as document collection to provide answers to user questions / search queries. First, we split all Wikipedia articles into paragraphs and encode them with a bi-encoder. If a new query / question is entered, it is encoded by the same bi-encoder and the paragraphs with the highest cosine-similarity are retrieved (see <a class="reference internal" href="../semantic-search/README.html"><span class="doc">semantic search</span></a>). Next, the retrieved candidates are scored by a Cross-Encoder re-ranker and the 5 passages with the highest score from the Cross-Encoder are presented to the user.</p></li>
</ul>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/retrieve_rerank/in_document_search_crossencoder.py">in_document_search_crossencoder.py</a>:</strong> If have only have a small set of paragraphs, we donâ€™t the retrieval stage. This is for example the case if you want to perform search within a single document. In this example, take the Wikipedia article about Europe and split it into paragraphs. Then, the search query / question and all paragraphs are scored using the Cross-Encoder re-ranker. The most relevant passages for the query are returned.</p></li>
</ul>
</div>
<div class="section" id="pre-trained-bi-encoders-retrieval">
<h2>Pre-trained Bi-Encoders (Retrieval)<a class="headerlink" href="#pre-trained-bi-encoders-retrieval" title="Permalink to this headline">Â¶</a></h2>
<p>The bi-encoder produces embeddings independently for your paragraphs and for your search queries. You can use it like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;model_name&#39;</span><span class="p">)</span>

<span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;My first paragraph. That contains information&quot;</span><span class="p">,</span> <span class="s2">&quot;Python is a programming language.&quot;</span><span class="p">]</span>
<span class="n">document_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;What is Python?&quot;</span>
<span class="n">query_embedding</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
</pre></div>
</div>
<p>For more details how to compare the embeddings, see <a class="reference internal" href="../semantic-search/README.html"><span class="doc">semantic search</span></a>.</p>
<p>We provide pre-trained models based on:</p>
<ul class="simple">
<li><p><strong>MS MARCO:</strong> 500k real user queries from Bing search engine. See <a class="reference external" href="https://www.sbert.net/docs/pretrained-models/msmarco-v3.html">MS MARCO models</a></p></li>
</ul>
</div>
<div class="section" id="pre-trained-cross-encoders-re-ranker">
<h2>Pre-trained Cross-Encoders (Re-Ranker)<a class="headerlink" href="#pre-trained-cross-encoders-re-ranker" title="Permalink to this headline">Â¶</a></h2>
<p>For pre-trained models, see: <a class="reference external" href="https://www.sbert.net/docs/pretrained-models/ce-msmarco.html">MS MARCO Cross-Encoders</a></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../clustering/README.html" class="btn btn-neutral float-right" title="Clustering" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../semantic-search/README.html" class="btn btn-neutral float-left" title="Semantic Search" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2022, Nils Reimers

       &bull; <a href="/docs/contact.html">Contact</a>

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>