<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<html>
 <head>
  <title>Index of /examples/applications/semantic-search</title>
 </head>
 <body>
<h1>Index of /examples/applications/semantic-search</h1>
  <table>
   <tr><th valign="top"><img src="/icons/blank.gif" alt="[ICO]"></th><th><a href="?C=N;O=D">Name</a></th><th><a href="?C=M;O=A">Last modified</a></th><th><a href="?C=S;O=A">Size</a></th><th><a href="?C=D;O=A">Description</a></th></tr>
   <tr><th colspan="5"><hr></th></tr>
<tr><td valign="top"><img src="/icons/back.gif" alt="[PARENTDIR]"></td><td><a href="/examples/applications/">Parent Directory</a></td><td>&nbsp;</td><td align="right">  - </td><td>&nbsp;</td></tr>
<tr><td valign="top"><img src="/icons/text.gif" alt="[TXT]"></td><td><a href="README.html">README.html</a></td><td align="right">2022-06-30 21:32  </td><td align="right"> 37K</td><td>&nbsp;</td></tr>
   <tr><th colspan="5"><hr></th></tr>
</table>


<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Semantic Search &mdash; Sentence-Transformers  documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
  
  
  
    <link rel="canonical" href="https://www.sbert.netexamples/applications/semantic-search/README.html"/>
  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/js/custom.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Retrieve &amp; Re-Rank" href="../retrieve_rerank/README.html" />
    <link rel="prev" title="Semantic Textual Similarity" href="../../../docs/usage/semantic_textual_similarity.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

            <a href="../../../index.html">
              <img src="../../../_static/logo.png" class="logo" alt="Logo"/>
              <span class="icon icon-home project-name"> Sentence-Transformers</span>
            </a>

            <div style="display: flex; justify-content: center;">
              <div id="twitter-button">
                <a href="https://twitter.com/Nils_Reimers" target="_blank" title="Follow SBERT on Twitter"><img src="/_static/Twitter_Logo_White.svg" height="20" style="margin: 0px 10px 0px -10px;"> </a>
              </div>
              <div id="github-button"></div>
            </div>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/pretrained_models.html">Pretrained Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/pretrained_cross-encoders.html">Pretrained Cross-Encoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/hugging_face.html">Hugging Face 🤗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Usage</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../computing-embeddings/README.html">Computing Sentence Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/usage/semantic_textual_similarity.html">Semantic Textual Similarity</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Semantic Search</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#background">Background</a></li>
<li class="toctree-l2"><a class="reference internal" href="#symmetric-vs-asymmetric-semantic-search">Symmetric vs. Asymmetric Semantic Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="#python">Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="#util-semantic-search">util.semantic_search</a></li>
<li class="toctree-l2"><a class="reference internal" href="#speed-optimization">Speed Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#elasticsearch">ElasticSearch</a></li>
<li class="toctree-l2"><a class="reference internal" href="#approximate-nearest-neighbor">Approximate Nearest Neighbor</a></li>
<li class="toctree-l2"><a class="reference internal" href="#retrieve-re-rank">Retrieve &amp; Re-Rank</a></li>
<li class="toctree-l2"><a class="reference internal" href="#examples">Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#similar-questions-retrieval">Similar Questions Retrieval</a></li>
<li class="toctree-l3"><a class="reference internal" href="#similar-publication-retrieval">Similar Publication Retrieval</a></li>
<li class="toctree-l3"><a class="reference internal" href="#question-answer-retrieval">Question &amp; Answer Retrieval</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../retrieve_rerank/README.html">Retrieve &amp; Re-Rank</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clustering/README.html">Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../paraphrase-mining/README.html">Paraphrase Mining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel-sentence-mining/README.html">Translated Sentence Mining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cross-encoder/README.html">Cross-Encoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../image-search/README.html">Image Search</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/training/overview.html">Training Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/multilingual/README.html">Multilingual-Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/distillation/README.html">Model Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/cross-encoder/README.html">Cross-Encoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/data_augmentation/README.html">Augmented SBERT</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../training/sts/README.html">Semantic Textual Similarity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/nli/README.html">Natural Language Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/paraphrases/README.html">Paraphrase Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/quora_duplicate_questions/README.html">Quora Duplicate Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/ms_marco/README.html">MS MARCO</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Unsupervised Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../unsupervised_learning/README.html">Unsupervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../domain_adaptation/README.html">Domain Adaptation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/SentenceTransformer.html">SentenceTransformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/util.html">util</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/losses.html">Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/evaluation.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/datasets.html">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/cross_encoder.html">cross_encoder</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Sentence-Transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Semantic Search</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            
              <a href="https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/semantic-search/README.md" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="semantic-search">
<h1>Semantic Search<a class="headerlink" href="#semantic-search" title="Permalink to this headline">¶</a></h1>
<p>Semantic search seeks to improve search accuracy by understanding the content of the search query. In contrast to traditional search engines which only find documents based on lexical matches, semantic search can also find synonyms.</p>
<div class="section" id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<p>The idea behind semantic search is to embed all entries in your corpus, whether they be sentences, paragraphs, or documents, into a vector space.</p>
<p>At search time, the query is embedded into the same vector space and the closest embeddings from your corpus are found. These entries should have a high semantic overlap with the query.</p>
<p><img alt="SemanticSearch" src="https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/SemanticSearch.png" /></p>
</div>
<div class="section" id="symmetric-vs-asymmetric-semantic-search">
<h2>Symmetric vs. Asymmetric Semantic Search<a class="headerlink" href="#symmetric-vs-asymmetric-semantic-search" title="Permalink to this headline">¶</a></h2>
<p>A <strong>critical distinction</strong> for your setup is <em>symmetric</em> vs. <em>asymmetric semantic search</em>:</p>
<ul class="simple">
<li><p>For <strong>symmetric semantic search</strong> your query and the entries in your corpus are of about the same length and have the same amount of content. An example would be searching for similar questions: Your query could for example be <em>“How to learn Python online?”</em> and you want to find an entry like <em>“How to learn Python on the web?”</em>. For symmetric tasks, you could potentially flip the query and the entries in your corpus.</p></li>
<li><p>For <strong>asymmetric semantic search</strong>, you usually have a <strong>short query</strong> (like a question or some keywords) and you want to find a longer paragraph answering the query. An example would be a query like <em>“What is Python”</em> and you wand to find the paragraph <em>“Python is an interpreted, high-level and general-purpose programming language. Python’s design philosophy …”</em>. For asymmetric tasks, flipping the query and the entries in your corpus usually does not make sense.</p></li>
</ul>
<p>It is critical <strong>that you choose the right model</strong> for your type of task.</p>
<p>Suitable models for <strong>symmetric semantic search</strong>: <a class="reference external" href="https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models">Pre-Trained Sentence Embedding Models</a></p>
<p>Suitable models for <strong>asymmetric semantic search</strong>: <a class="reference external" href="https://www.sbert.net/docs/pretrained-models/msmarco-v3.html">Pre-Trained MS MARCO Models</a></p>
</div>
<div class="section" id="python">
<h2>Python<a class="headerlink" href="#python" title="Permalink to this headline">¶</a></h2>
<p>For small corpora (up to about 1 million entries) we can compute the cosine-similarity between the query and all entries in the corpus.</p>
<p>In the following example, we define a small corpus with few example sentences and compute the embeddings for the corpus as well as for our query.</p>
<p>We then use the <a class="reference internal" href="../../../docs/usage/semantic_textual_similarity.html"><span class="doc">util.cos_sim()</span></a> function to compute the cosine similarity between the query and all corpus entries.</p>
<p>For large corpora, sorting all scores would take too much time. Hence, we use <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.topk.html">torch.topk</a> to only get the top k entries.</p>
<p>For a simple example, see <a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search.py">semantic_search.py</a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This is a simple application for sentence embeddings: semantic search</span>

<span class="sd">We have a corpus with various sentences. Then, for a given query sentence,</span>
<span class="sd">we want to find the most similar sentence in this corpus.</span>

<span class="sd">This script outputs for various queries the top 5 most similar sentences in the corpus.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">util</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">embedder</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;all-MiniLM-L6-v2&#39;</span><span class="p">)</span>

<span class="c1"># Corpus with example sentences</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;A man is eating food.&#39;</span><span class="p">,</span>
          <span class="s1">&#39;A man is eating a piece of bread.&#39;</span><span class="p">,</span>
          <span class="s1">&#39;The girl is carrying a baby.&#39;</span><span class="p">,</span>
          <span class="s1">&#39;A man is riding a horse.&#39;</span><span class="p">,</span>
          <span class="s1">&#39;A woman is playing violin.&#39;</span><span class="p">,</span>
          <span class="s1">&#39;Two men pushed carts through the woods.&#39;</span><span class="p">,</span>
          <span class="s1">&#39;A man is riding a white horse on an enclosed ground.&#39;</span><span class="p">,</span>
          <span class="s1">&#39;A monkey is playing drums.&#39;</span><span class="p">,</span>
          <span class="s1">&#39;A cheetah is running behind its prey.&#39;</span>
          <span class="p">]</span>
<span class="n">corpus_embeddings</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">convert_to_tensor</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Query sentences:</span>
<span class="n">queries</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;A man is eating pasta.&#39;</span><span class="p">,</span> <span class="s1">&#39;Someone in a gorilla costume is playing a set of drums.&#39;</span><span class="p">,</span> <span class="s1">&#39;A cheetah chases prey on across a field.&#39;</span><span class="p">]</span>


<span class="c1"># Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity</span>
<span class="n">top_k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">corpus</span><span class="p">))</span>
<span class="k">for</span> <span class="n">query</span> <span class="ow">in</span> <span class="n">queries</span><span class="p">:</span>
    <span class="n">query_embedding</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">convert_to_tensor</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># We use cosine-similarity and torch.topk to find the highest 5 scores</span>
    <span class="n">cos_scores</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">cos_sim</span><span class="p">(</span><span class="n">query_embedding</span><span class="p">,</span> <span class="n">corpus_embeddings</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">top_results</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">cos_scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">======================</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Query:&quot;</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Top 5 most similar sentences in corpus:&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">score</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">top_results</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">top_results</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">corpus</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="s2">&quot;(Score: </span><span class="si">{:.4f}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="p">))</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk</span>
<span class="sd">    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)</span>
<span class="sd">    hits = hits[0]      #Get the hits for the first query</span>
<span class="sd">    for hit in hits:</span>
<span class="sd">        print(corpus[hit[&#39;corpus_id&#39;]], &quot;(Score: {:.4f})&quot;.format(hit[&#39;score&#39;]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="util-semantic-search">
<h2>util.semantic_search<a class="headerlink" href="#util-semantic-search" title="Permalink to this headline">¶</a></h2>
<p>Instead of implementing semantic search by yourself, you can use the <em>util.semantic_search</em> function.</p>
<p>The function accepts the following parameters:</p>
<dl class="py function">
<dt class="sig sig-object py" id="sentence_transformers.util.semantic_search">
<span class="sig-prename descclassname"><span class="pre">sentence_transformers.util.</span></span><span class="sig-name descname"><span class="pre">semantic_search</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">query_embeddings:</span> <span class="pre">torch.Tensor,</span> <span class="pre">corpus_embeddings:</span> <span class="pre">torch.Tensor,</span> <span class="pre">query_chunk_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">100,</span> <span class="pre">corpus_chunk_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">500000,</span> <span class="pre">top_k:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">10,</span> <span class="pre">score_function:</span> <span class="pre">typing.Callable[[torch.Tensor,</span> <span class="pre">torch.Tensor],</span> <span class="pre">torch.Tensor]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">cos_sim&gt;</span></em><span class="sig-paren">)</span><a class="headerlink" href="#sentence_transformers.util.semantic_search" title="Permalink to this definition">¶</a></dt>
<dd><p>This function performs a cosine similarity search between a list of query embeddings  and a list of corpus embeddings.
It can be used for Information Retrieval / Semantic Search for corpora up to about 1 Million entries.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query_embeddings</strong> – A 2 dimensional tensor with the query embeddings.</p></li>
<li><p><strong>corpus_embeddings</strong> – A 2 dimensional tensor with the corpus embeddings.</p></li>
<li><p><strong>query_chunk_size</strong> – Process 100 queries simultaneously. Increasing that value increases the speed, but requires more memory.</p></li>
<li><p><strong>corpus_chunk_size</strong> – Scans the corpus 100k entries at a time. Increasing that value increases the speed, but requires more memory.</p></li>
<li><p><strong>top_k</strong> – Retrieve top k matching entries.</p></li>
<li><p><strong>score_function</strong> – Function for computing scores. By default, cosine similarity.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns a list with one entry for each query. Each entry is a list of dictionaries with the keys ‘corpus_id’ and ‘score’, sorted by decreasing cosine similarity scores.</p>
</dd>
</dl>
</dd></dl>

<p>By default, up to 100 queries are processed in parallel. Further, the corpus is chunked into set of up to 500k entries. You can increase <em>query_chunk_size</em> and <em>corpus_chunk_size</em>, which leads to increased speed for large corpora, but also increases the memory requirement.</p>
</div>
<div class="section" id="speed-optimization">
<h2>Speed Optimization<a class="headerlink" href="#speed-optimization" title="Permalink to this headline">¶</a></h2>
<p>To get the optimal speed for the <code class="docutils literal notranslate"><span class="pre">util.semantic_search</span></code> method, it is advisable to have the <code class="docutils literal notranslate"><span class="pre">query_embeddings</span></code> as well as the <code class="docutils literal notranslate"><span class="pre">corpus_embeddings</span></code> on the same GPU-device. This significantly boost the performance.</p>
<p>Further, we can normalize the corpus embeddings so that each corpus embeddings is of length 1. In that case, we can use dot-product for computing scores.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">corpus_embeddings</span> <span class="o">=</span> <span class="n">corpus_embeddings</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">corpus_embeddings</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">normalize_embeddings</span><span class="p">(</span><span class="n">corpus_embeddings</span><span class="p">)</span>

<span class="n">query_embeddings</span> <span class="o">=</span> <span class="n">query_embeddings</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">query_embeddings</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">normalize_embeddings</span><span class="p">(</span><span class="n">query_embeddings</span><span class="p">)</span>
<span class="n">hits</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">semantic_search</span><span class="p">(</span><span class="n">query_embeddings</span><span class="p">,</span> <span class="n">corpus_embeddings</span><span class="p">,</span> <span class="n">score_function</span><span class="o">=</span><span class="n">util</span><span class="o">.</span><span class="n">dot_score</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="elasticsearch">
<h2>ElasticSearch<a class="headerlink" href="#elasticsearch" title="Permalink to this headline">¶</a></h2>
<p>Starting with version 7.3, <a class="reference external" href="https://www.elastic.co/elasticsearch/">ElasticSearch</a> introduced the possibility to index dense vectors and to use to for document scoring. Hence, we can use ElasticSearch to index embeddings along the documents and we can use the query embeddings to retrieve relevant entries.</p>
<p>An advantage of ElasticSearch is that it is easy to add new documents to an index and that we can store also other data along with our vectors. A disadvantage is the slow performance, as it compares the query embeddings with all stored embeddings. This has a linear run-time and might be too slow for large (&gt;100k) corpora.</p>
<p>For further details, see <a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_elasticsearch.py">semantic_search_quora_elasticsearch.py</a>.</p>
</div>
<div class="section" id="approximate-nearest-neighbor">
<h2>Approximate Nearest Neighbor<a class="headerlink" href="#approximate-nearest-neighbor" title="Permalink to this headline">¶</a></h2>
<p>Searching a large corpus with millions of embeddings can be time-consuming if exact nearest neighbor search is used (like it is used by <em>util.semantic_search</em>).</p>
<p>In that case, Approximate Nearest Neighor (ANN) can be helpful. Here, the data is partitioned into smaller fractions of similar embeddings. This index can be searched efficiently and the embeddings with the highest similarity (the nearest neighbors) can be retrieved within milliseconds, even if you have millions of vectors.</p>
<p>However, the results are not necessarily exact. It is possible that some vectors with high similarity will be missed. That’s the reason why it is called approximate nearest neighbor.</p>
<p>For all ANN methods, there are usually one or more parameters to tune that determine the recall-speed trade-off. If you want the highest speed, you have a high chance of missing hits. If you want high recall, the search speed decreases.</p>
<p>Three popular libraries for approximate nearest neighbor are <a class="reference external" href="https://github.com/spotify/annoy">Annoy</a>, <a class="reference external" href="https://github.com/facebookresearch/faiss">FAISS</a>, and <a class="reference external" href="https://github.com/nmslib/hnswlib/">hnswlib</a>. Personally I find hnswlib the most suitable library: It is easy to use, offers a great performance and has nice features included that are important for real applications.</p>
<p>Examples:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_hnswlib.py">semantic_search_quora_hnswlib.py</a></p></li>
<li><p><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_annoy.py">semantic_search_quora_annoy.py</a></p></li>
<li><p><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_faiss.py">semantic_search_quora_faiss.py</a></p></li>
</ul>
</div>
<div class="section" id="retrieve-re-rank">
<h2>Retrieve &amp; Re-Rank<a class="headerlink" href="#retrieve-re-rank" title="Permalink to this headline">¶</a></h2>
<p>For complex semantic search scenarios, a retrieve &amp; re-rank pipeline is advisable:
<img alt="InformationRetrieval" src="https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/InformationRetrieval.png" /></p>
<p>For further details, see <a class="reference internal" href="../retrieve_rerank/README.html"><span class="doc">Retrieve &amp; Re-rank</span></a>.</p>
</div>
<div class="section" id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h2>
<p>In the following we list examples for different use-cases.</p>
<div class="section" id="similar-questions-retrieval">
<h3>Similar Questions Retrieval<a class="headerlink" href="#similar-questions-retrieval" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_pytorch.py">semantic_search_quora_pytorch.py</a> [ <a class="reference external" href="https://colab.research.google.com/drive/12cn5Oo0v3HfQQ8Tv6-ukgxXSmT3zl35A?usp=sharing">Colab version</a> ] shows an example based on the <a class="reference external" href="https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs">Quora duplicate questions</a> dataset. The user can enter a question, and the code retrieves the most similar questions from the dataset using the <em>util.semantic_search</em> method. As model, we use <em>distilbert-multilingual-nli-stsb-quora-ranking</em>, which was trained to identify similar questions and supports 50+ languages. Hence, the user can input the question in any of the 50+ languages. This is a <strong>symmetric search task</strong>, as the search queries have the same length and content as the questions in the corpus.</p>
</div>
<div class="section" id="similar-publication-retrieval">
<h3>Similar Publication Retrieval<a class="headerlink" href="#similar-publication-retrieval" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_publications.py">semantic_search_publications.py</a> [ <a class="reference external" href="https://colab.research.google.com/drive/12hfBveGHRsxhPIUMmJYrll2lFU4fOX06?usp=sharing">Colab version</a> ] shows an example how to find similar scientific publications. As corpus, we use all publications that have been presented at the EMNLP 2016 - 2018 conferences. As search query, we input the title and abstract of more recent publications and find related publications from our copurs. We use the <a class="reference external" href="https://arxiv.org/abs/2004.07180">SPECTER</a> model. This is a <strong>symmetric search task</strong>, as the paper in the corpus consists of title &amp; abstract and we search for title &amp; abstract.</p>
</div>
<div class="section" id="question-answer-retrieval">
<h3>Question &amp; Answer Retrieval<a class="headerlink" href="#question-answer-retrieval" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_wikipedia_qa.py">semantic_search_wikipedia_qa.py</a> [ <a class="reference external" href="https://colab.research.google.com/drive/11GunvCqJuebfeTlgbJWkIMT0xJH6PWF1?usp=sharing">Colab Version</a> ]: This example uses a model that was trained on the <a class="reference external" href="https://ai.google.com/research/NaturalQuestions/">Natural Questions dataset</a>. It consists of about 100k real Google search queries, together with an annotated passage from Wikipedia that provides the answer. It is an example of an <strong>asymmetric search task</strong>. As corpus, we use the smaller <a class="reference external" href="https://simple.wikipedia.org/wiki/Main_Page">Simple English Wikipedia</a> so that it fits easily into memory.</p>
<p><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/../retrieve_rerank/retrieve_rerank_simple_wikipedia.py">retrieve_rerank_simple_wikipedia.py</a> [ <a class="reference external" href="https://colab.research.google.com/github/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb">Colab Version</a> ]: This script uses the <a class="reference internal" href="../retrieve_rerank/README.html"><span class="doc">Retrieve &amp; Re-rank</span></a> strategy and is an example for an <strong>asymmetric search task</strong>. We split all Wikipedia articles into paragraphs and encode them with a bi-encoder. If a new query / question is entered, it is encoded by the same bi-encoder and the paragraphs with the highest cosine-similarity are retrieved (see <a class="reference internal" href="#"><span class="doc">semantic search</span></a>). Next, the retrieved candidates are scored by a Cross-Encoder re-ranker and the 5 passages with the highest score from the Cross-Encoder are presented to the user. We use models that were trained on the <a class="reference external" href="https://github.com/microsoft/MSMARCO-Passage-Ranking/">MS Marco Passage Reranking</a> dataset, a dataset with about 500k real queries from Bing search.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../retrieve_rerank/README.html" class="btn btn-neutral float-right" title="Retrieve &amp; Re-Rank" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../../../docs/usage/semantic_textual_similarity.html" class="btn btn-neutral float-left" title="Semantic Textual Similarity" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2022, Nils Reimers

       &bull; <a href="/docs/contact.html">Contact</a>

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html></body></html>
