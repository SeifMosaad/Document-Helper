<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<html>
 <head>
  <title>Index of /examples/applications/information-retrieval</title>
 </head>
 <body>
<h1>Index of /examples/applications/information-retrieval</h1>
  <table>
   <tr><th valign="top"><img src="/icons/blank.gif" alt="[ICO]"></th><th><a href="?C=N;O=D">Name</a></th><th><a href="?C=M;O=A">Last modified</a></th><th><a href="?C=S;O=A">Size</a></th><th><a href="?C=D;O=A">Description</a></th></tr>
   <tr><th colspan="5"><hr></th></tr>
<tr><td valign="top"><img src="/icons/back.gif" alt="[PARENTDIR]"></td><td><a href="/examples/applications/">Parent Directory</a></td><td>&nbsp;</td><td align="right">  - </td><td>&nbsp;</td></tr>
<tr><td valign="top"><img src="/icons/text.gif" alt="[TXT]"></td><td><a href="README.html">README.html</a></td><td align="right">2022-06-30 21:32  </td><td align="right"> 22K</td><td>&nbsp;</td></tr>
   <tr><th colspan="5"><hr></th></tr>
</table>


<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Information Retrieval &mdash; Sentence-Transformers  documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
  
  
  
    <link rel="canonical" href="https://www.sbert.netexamples/applications/information-retrieval/README.html"/>
  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
        <script src="../../../_static/js/custom.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Cross-Encoders" href="../cross-encoder/README.html" />
    <link rel="prev" title="Semantic Search" href="../semantic-search/README.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

            <a href="../../../index.html">
              <img src="../../../_static/logo.png" class="logo" alt="Logo"/>
              <span class="icon icon-home project-name"> Sentence-Transformers</span>
            </a>

            <div style="display: flex; justify-content: center;">
              <div id="twitter-button">
                <a href="https://twitter.com/Nils_Reimers" target="_blank" title="Follow SBERT on Twitter"><img src="https://sbert.net/_static/Twitter_Logo_White.svg" height="20" style="margin: 0px 10px 0px -10px;"> </a>
              </div>
              <div id="github-button"></div>
            </div>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/pretrained_models.html">Pretrained Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/pretrained_cross-encoders.html">Pretrained Cross-Encoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/publications.html">Publications</a></li>
</ul>
<p class="caption"><span class="caption-text">Usage</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../computing-embeddings/README.html">Computing Sentence Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/usage/semantic_textual_similarity.html">Semantic Textual Similarity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clustering/README.html">Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../paraphrase-mining/README.html">Paraphrase Mining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel-sentence-mining/README.html">Translated Sentence Mining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../semantic-search/README.html">Semantic Search</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Information Retrieval</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#retrieval-pipeline">Retrieval Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="#retrieval-bi-encoder">Retrieval: Bi-Encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#re-ranker-cross-encoder">Re-Ranker: Cross-Encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#example-scripts">Example Scripts</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pre-trained-bi-encoders-retrieval">Pre-trained Bi-Encoders (Retrieval)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pre-trained-cross-encoders-re-ranker">Pre-trained Cross-Encoders (Re-ranker)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../cross-encoder/README.html">Cross-Encoders</a></li>
</ul>
<p class="caption"><span class="caption-text">Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/training/overview.html">Training Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/multilingual/README.html">Multilingual-Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/distillation/README.html">Model Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/cross-encoder/README.html">Cross-Encoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/data_augmentation/README.html">Augmented SBERT</a></li>
</ul>
<p class="caption"><span class="caption-text">Training Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../training/sts/README.html">Semantic Textual Similarity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/nli/README.html">Natural Language Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/quora_duplicate_questions/README.html">Quora Duplicate Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/ms_marco/README.html">MS Marco</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/SentenceTransformer.html">SentenceTransformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/util.html">util</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/losses.html">Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/evaluation.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/datasets.html">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/package_reference/cross_encoder.html">cross_encoder</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Sentence-Transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Information Retrieval</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            
              <a href="https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/information-retrieval/README.md" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="information-retrieval">
<h1>Information Retrieval<a class="headerlink" href="#information-retrieval" title="Permalink to this headline">¶</a></h1>
<p>Given a <em>query</em>, we will search a large collection for matching documents.</p>
<p>This articles focuses on the case where <em>query</em> is a search query or a question and the documents are paragraphs answering the search query / question. Also have look at <a class="reference internal" href="../semantic-search/README.html"><span class="doc">semantic search</span></a> for the case where query and documents are of the same size.</p>
<div class="section" id="retrieval-pipeline">
<h2>Retrieval Pipeline<a class="headerlink" href="#retrieval-pipeline" title="Permalink to this headline">¶</a></h2>
<p>A pipeline for information retrieval / question answering retrieval that works well is the following. All components are provided and explained in this article:</p>
<p><img alt="InformationRetrieval" src="https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/InformationRetrieval.png" /></p>
<p>Given a search query, we first use a <strong>retrieval system</strong> that retrieves a large list of e.g. 100 possible hits which are potentially relevant for the query. For the retrieval, we can use either lexical search, e.g. with ElasticSearch, or we can use dense retrieval with a bi-encoder.</p>
<p>However, the retrieval system might retrieve documents that are not that relevant for the search query. Hence, in a second stage, we use a <strong>re-ranker</strong> based on a <strong>cross-encoder</strong> that scores the relevancy of all candidates for the given search query.</p>
<p>The output will be a ranked list of hits we can present to the user.</p>
</div>
<div class="section" id="retrieval-bi-encoder">
<h2>Retrieval: Bi-Encoder<a class="headerlink" href="#retrieval-bi-encoder" title="Permalink to this headline">¶</a></h2>
<p>For the retrieval of the candidate set, we can either use lexical search, e.g. with <a class="reference external" href="https://www.elastic.co/elasticsearch/">ElasticSearch</a>, or we can use a bi-encoder which is implemented in this repository.</p>
<p>Lexical search looks for literal matches of the query words in your document collection. It will not recognize synonyms, acronyms or spelling variations. In contrast, semantic search (or dense retrieval) encodes the search query into vector space and retrieves the document embeddings that are close in vector space.</p>
<p><img alt="SemanticSearch" src="https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/SemanticSearch.png" /></p>
<p>Semantic search overcomes the short comings of lexical search and can recognize synonym and acronyms. Have a look at the <a class="reference internal" href="../semantic-search/README.html"><span class="doc">semantic search article</span></a>  for different options to implement semantic search.</p>
</div>
<div class="section" id="re-ranker-cross-encoder">
<h2>Re-Ranker: Cross-Encoder<a class="headerlink" href="#re-ranker-cross-encoder" title="Permalink to this headline">¶</a></h2>
<p>The retriever has to be efficient for large document collections with millions of entries. However, it might return irrelevant candidates.</p>
<p>A re-ranker based on a Cross-Encoder can substantially improve the final results for the user. The query and a possible document is passed simultaneously to transformer network, which then outputs a single score between 0 and 1 indicating how relevant the document is for the given query.</p>
<p><img alt="CrossEncoder" src="https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/CrossEncoder.png" /></p>
<p>The advantage of Cross-Encoders is the higher performance, as they perform attention across the query and the document.</p>
<p>Scoring thousands or millions of (query, document)-pairs would be rather slow. Hence, we use the retriever to create a set of e.g. 100 possible candidates which are then re-ranked by the Cross-Encoder.</p>
</div>
<div class="section" id="example-scripts">
<h2>Example Scripts<a class="headerlink" href="#example-scripts" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/information-retrieval/qa_retrieval_simple_wikipedia.py">qa_retrieval_simple_wikipedia.py</a></strong> [ <a class="reference external" href="https://colab.research.google.com/drive/1l6stpYdRMmeDBK_vw0L5NitdiAuhdsAr?usp=sharing">Colab Version</a> ]: This script uses the smaller <a class="reference external" href="https://simple.wikipedia.org/wiki/Main_Page">Simple English Wikipedia</a> as document collection to provide answers to user questions / search queries. First, we split all Wikipedia articles into paragraphs and encode them with a bi-encoder. If a new query / question is entered, it is encoded by the same bi-encoder and the paragraphs with the highest cosine-similarity are retrieved (see <a class="reference internal" href="../semantic-search/README.html"><span class="doc">semantic search</span></a>). Next, the retrieved candidates are scored by a Cross-Encoder re-ranker and the 5 passages with the highest score from the Cross-Encoder are presented to the user.</p></li>
</ul>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/information-retrieval/in_document_search_crossencoder.py">in_document_search_crossencoder.py</a>:</strong> If have only have a small set of paragraphs, we don’t the retrieval stage. This is for example the case if you want to perform search within a single document. In this example, take the Wikipedia article about Europe and split it into paragraphs. Then, the search query / question and all paragraphs are scored using the Cross-Encoder re-ranker. The most relevant passages for the query are returned.</p></li>
</ul>
</div>
<div class="section" id="pre-trained-bi-encoders-retrieval">
<h2>Pre-trained Bi-Encoders (Retrieval)<a class="headerlink" href="#pre-trained-bi-encoders-retrieval" title="Permalink to this headline">¶</a></h2>
<p>The bi-encoder produces embeddings independently for your paragraphs and for your search queries. You can use it like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;model_name&#39;</span><span class="p">)</span>

<span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;My first paragraph. That contains information&quot;</span><span class="p">,</span> <span class="s2">&quot;Python is a programming language.&quot;</span><span class="p">]</span>
<span class="n">document_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;What is Python?&quot;</span>
<span class="n">query_embedding</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
</pre></div>
</div>
<p>For more details how to compare the embeddings, see <a class="reference internal" href="../semantic-search/README.html"><span class="doc">semantic search</span></a>.</p>
<p>We provide pre-trained models based on:</p>
<ul class="simple">
<li><p><strong>MS MARCO:</strong> 500k real user queries from Bing search engine. See <a class="reference external" href="https://www.sbert.net/docs/pretrained-models/msmarco-v2.html">MS MARCO models</a></p></li>
</ul>
</div>
<div class="section" id="pre-trained-cross-encoders-re-ranker">
<h2>Pre-trained Cross-Encoders (Re-ranker)<a class="headerlink" href="#pre-trained-cross-encoders-re-ranker" title="Permalink to this headline">¶</a></h2>
<p>Pre-trained models can be used like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">CrossEncoder</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CrossEncoder</span><span class="p">(</span><span class="s1">&#39;model_name&#39;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([(</span><span class="s1">&#39;Query&#39;</span><span class="p">,</span> <span class="s1">&#39;Paragraph1&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Query&#39;</span><span class="p">,</span> <span class="s1">&#39;Paragraph2&#39;</span><span class="p">)</span> <span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Query&#39;</span><span class="p">,</span> <span class="s1">&#39;Paragraph3&#39;</span><span class="p">)])</span>
</pre></div>
</div>
<p>In the following table, we provide various pre-trained Cross-Encoders together with their performance on the <a class="reference external" href="https://microsoft.github.io/TREC-2019-Deep-Learning/">TREC Deep Learning 2019</a> and the <a class="reference external" href="https://github.com/microsoft/MSMARCO-Passage-Ranking/">MS Marco Passage Reranking</a> dataset.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model-Name</th>
<th align="left">NDCG@10 (TREC DL 19)</th>
<th>MRR@10 (MS Marco Dev)</th>
<th>Docs / Sec (BertTokenizerFast)</th>
<th>Docs / Sec</th>
</tr>
</thead>
<tbody>
<tr>
<td>cross-encoder/ms-marco-TinyBERT-L-2</td>
<td align="left">67.43</td>
<td>30.15</td>
<td>9000</td>
<td>780</td>
</tr>
<tr>
<td>cross-encoder/ms-marco-TinyBERT-L-4</td>
<td align="left">68.09</td>
<td>34.50</td>
<td>2900</td>
<td>760</td>
</tr>
<tr>
<td>cross-encoder/ms-marco-TinyBERT-L-6</td>
<td align="left">69.57</td>
<td>36.13</td>
<td>680</td>
<td>660</td>
</tr>
<tr>
<td>cross-encoder/ms-marco-electra-base</td>
<td align="left">71.99</td>
<td>36.41</td>
<td>340</td>
<td>340</td>
</tr>
<tr>
<td><em>Other models</em></td>
<td align="left"></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>nboost/pt-tinybert-msmarco</td>
<td align="left">63.63</td>
<td>28.80</td>
<td>2900</td>
<td>760</td>
</tr>
<tr>
<td>nboost/pt-bert-base-uncased-msmarco</td>
<td align="left">70.94</td>
<td>34.75</td>
<td>340</td>
<td>340</td>
</tr>
<tr>
<td>nboost/pt-bert-large-msmarco</td>
<td align="left">73.36</td>
<td>36.48</td>
<td>100</td>
<td>100</td>
</tr>
<tr>
<td>Capreolus/electra-base-msmarco</td>
<td align="left">71.23</td>
<td>36.89</td>
<td>340</td>
<td>340</td>
</tr>
<tr>
<td>amberoad/bert-multilingual-passage-reranking-msmarco</td>
<td align="left">68.40</td>
<td>35.54</td>
<td>330</td>
<td>330</td>
</tr>
</tbody>
</table>
<p>Note: Runtime was computed on a V100 GPU. A bottleneck for smaller models is the Python-based tokenizer from Huggingface that was used in Huggingface Version 3. Starting with Huggingface Version 4, fast tokenizer based on Rust is used.</p></div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../cross-encoder/README.html" class="btn btn-neutral float-right" title="Cross-Encoders" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../semantic-search/README.html" class="btn btn-neutral float-left" title="Semantic Search" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Nils Reimers

       &bull; <a href="/docs/contact.html">Contact</a>

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html></body></html>
